{
  "architectures": [
    "INLLLMForCausalLM"
  ],
  "model_type": "inl-llm",
  "transformers_version": "4.57.0",
  "vocab_size": 50261,
  "d_model": 1728,
  "num_layers": 25,
  "num_heads": 32,
  "num_iterations_per_layer": 5,
  "feedforward_dim": 6912,
  "max_seq_len": 2048,
  "dropout": 0.1,
  "bos_token_id": 50256,
  "eos_token_id": 50256,
  "pad_token_id": 50256,
  "unk_token_id": 50256,
  "use_lowrank_embeddings": true,
  "lowrank_ratio": 0.125,
  "use_gradient_checkpointing": true,
  "use_shared_controllers": true,
  "use_adaptive_stopping": true,
  "adaptive_convergence_threshold": 0.001,
  "hierarchical_group_size": 64,
  "excitation_sparsity": 0.1,
  "dtype": "bfloat16",
  "use_cache": false,
  "initializer_range": 0.02,
  "integrator_type": "ultra_optimized",
  "controller_type": "shared",
  "equilibrium_type": "hierarchical",
  "excitation_type": "sparse_harmonic",
  "auto_map": {
    "AutoConfig": "modeling_inl_llm.INLLLMConfig",
    "AutoModelForCausalLM": "modeling_inl_llm.INLLLMForCausalLM"
  },
  "torch_dtype": "bfloat16"
}